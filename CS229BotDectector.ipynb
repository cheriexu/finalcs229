{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bGcJCjeXFZbz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /home/shared/anaconda3/lib/python3.6/site-packages\n",
      "Requirement already satisfied: keras in /home/shared/anaconda3/lib/python3.6/site-packages\n",
      "Requirement already satisfied: numpy in /home/shared/anaconda3/lib/python3.6/site-packages\n",
      "Requirement already satisfied: xgboost in /home/shared/anaconda3/lib/python3.6/site-packages\n",
      "Requirement already satisfied: sklearn in /home/shared/anaconda3/lib/python3.6/site-packages\n",
      "Requirement already satisfied: nltk in /home/shared/anaconda3/lib/python3.6/site-packages\n",
      "Requirement already satisfied: textblob in /home/shared/anaconda3/lib/python3.6/site-packages\n",
      "Requirement already satisfied: python-dateutil>=2 in /home/shared/anaconda3/lib/python3.6/site-packages (from pandas)\n",
      "Requirement already satisfied: pytz>=2011k in /home/shared/anaconda3/lib/python3.6/site-packages (from pandas)\n",
      "Requirement already satisfied: scipy>=0.14 in /home/shared/anaconda3/lib/python3.6/site-packages (from keras)\n",
      "Requirement already satisfied: pyyaml in /home/shared/anaconda3/lib/python3.6/site-packages (from keras)\n",
      "Requirement already satisfied: six>=1.9.0 in /home/shared/anaconda3/lib/python3.6/site-packages (from keras)\n",
      "Requirement already satisfied: scikit-learn in /home/shared/anaconda3/lib/python3.6/site-packages (from sklearn)\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas keras numpy xgboost sklearn nltk textblob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "convk9PYFt6p"
   },
   "source": [
    "# Get genuine human tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TcyRaSYtsqwh",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shared/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.test.gpu_device_name() #We good using GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/cheriexu/finalcs229\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rya8yaGT_-Ht"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "\n",
    "import pandas as pd, xgboost, numpy as np, textblob, string\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import layers, models, optimizers\n",
    "\n",
    "real_users = pd.read_csv('/home/cheriexu/genuine_accounts/users.csv',low_memory = False)\n",
    "real_tweets = pd.read_csv('/home/cheriexu/genuine_accounts/tweets.csv',low_memory = False)\n",
    "real_df = real_tweets.merge(real_users, on='id',how = 'outer', suffixes = ('_tweets','_users')) #Merging the user and tweets\n",
    "real_df['labels']=pd.Series(np.ones(len(real_df['id']))) #Creating a new column to indicate that this is human data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o-3RBOSS_fwp"
   },
   "source": [
    "CANCELED: Get bot data to supplement class imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kQEzdJPmzkIJ"
   },
   "source": [
    "Import Bot Dataset from this point on \n",
    "(https://github.com/fivethirtyeight/russian-troll-tweets/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dwpQbQrvoAxw"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "directory = os.fsencode('/home/cheriexu/russian-troll')\n",
    "troll_data = pd.DataFrame()\n",
    "for file in os.listdir(directory):\n",
    "  print(file)\n",
    "  filename = os.fsdecode(file)\n",
    "  if filename.endswith(\".csv\"):\n",
    "    botdata = pd.read_csv('/home/cheriexu/russian-troll/'+filename,low_memory = False)\n",
    "    troll_data = troll_data.append(botdata) #append together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bdCjl1rEug7x"
   },
   "outputs": [],
   "source": [
    "frac = 10\n",
    "troll_data['labels']=pd.Series(np.zeros(len(botdata)))\n",
    "#Limiting too only English language tweets\n",
    "english_bot = troll_data[troll_data['language']  == 'English']\n",
    "troll_data['language'].unique()\n",
    "print(len(english_bot),len(troll_data)) #Class imbalance though, we need more spam bot results\n",
    "train_df = pd.DataFrame() #Create train data frame for splitting later\n",
    "train_df = train_df.append(real_df[['text','labels']].sample(int(len(real_df)/frac)))\n",
    "english_bot = english_bot.rename(index=str, columns={\"content\": \"text\"})\n",
    "train_df = train_df.append(english_bot[['text','labels']].sample(int(len(english_bot)/frac)))\n",
    "train_df['text']= train_df['text'].str.lower()\n",
    "#Downsample use 10%\n",
    "len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SGFE7ANbr2AE"
   },
   "outputs": [],
   "source": [
    "#troll_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "plG4IxOc8OLd"
   },
   "source": [
    "Split train test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6qk21OCf8Nee"
   },
   "outputs": [],
   "source": [
    "# split the dataset into training and validation datasets \n",
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(train_df['text'], train_df['labels'], test_size = .2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "--xxycI-8Ufm"
   },
   "source": [
    "Create Count Vectors for NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RFT2IwTl8XHz"
   },
   "outputs": [],
   "source": [
    "# Create CountVectorizer\n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "#fill in nan to make sklearn happy\n",
    "train_df['text'] = train_df['text'].fillna(' ')\n",
    "train_x = train_x.fillna(' ')\n",
    "valid_x = valid_x.fillna(' ') \n",
    "count_vect.fit(train_df['text'])\n",
    "# transform the training and validation data using count vectorizer object\n",
    "xtrain_count =  count_vect.transform(train_x)\n",
    "xvalid_count =  count_vect.transform(valid_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JBOfthG40QGc"
   },
   "outputs": [],
   "source": [
    "print(len(train_x),len(valid_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H3WgYHt29Bf9"
   },
   "outputs": [],
   "source": [
    "#xtrain_count.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_kIvH_xv3LOj"
   },
   "source": [
    "Train Model of select type and make predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VndDnk8G3Joq"
   },
   "outputs": [],
   "source": [
    "def train_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "    pred_train = classifier.predict(feature_vector_train)\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    if is_neural_net:\n",
    "        predictions = predictions.argmax(axis=-1)\n",
    "\n",
    "    return metrics.classification_report(label, pred_train), metrics.classification_report(valid_y, predictions)#confusion matrix on validation matrix\n",
    "  #metrics.confusion_matrix(label, pred_test), metrics.confusion_matrix(valid_y, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WPRaf6ws8nNE"
   },
   "source": [
    "Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y3gHFXbN6j4w"
   },
   "outputs": [],
   "source": [
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "valid_y = encoder.fit_transform(valid_y)\n",
    "valid_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hs8oEaeF8oLG"
   },
   "outputs": [],
   "source": [
    "# Naive Bayes on Count Vectors\n",
    "np.nan_to_num(xtrain_count)\n",
    "np.nan_to_num(xvalid_count)\n",
    "conf_test, conf_valid = train_model(naive_bayes.MultinomialNB(), xtrain_count, train_y, xvalid_count)\n",
    "print (conf_test[0:2])\n",
    "print(conf_valid[0:2])\n",
    "\n",
    "'''\n",
    "\n",
    "# Naive Bayes on Ngram Level TF IDF Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "print \"NB, N-Gram Vectors: \", accuracy\n",
    "\n",
    "# Naive Bayes on Character Level TF IDF Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6YNG6mAPtjUv"
   },
   "outputs": [],
   "source": [
    "print (conf_test)\n",
    "print(conf_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xeveKrNAze8d"
   },
   "source": [
    "##Naive Bayes on Count Vector Classification Results\n",
    "###         Train: n = 49596\n",
    "\n",
    "                  precision   recall  f1-score   support\n",
    "\n",
    "          0       0.90       0.91      0.90     20069\n",
    "          1       0.93      0.96      0.95     28448\n",
    "          \n",
    "          avg / total       0.90      0.92      0.91     49596\n",
    "          \n",
    "###          Valid: n = 12399\n",
    "  \n",
    "          \n",
    "           precision    recall  f1-score   support\n",
    "\n",
    "          0       0.83      0.85      0.84      5047\n",
    "          1       0.89      0.91      0.90      7087\n",
    "    avg / total       0.85      0.87      0.86     12399\n",
    "          \n",
    "          \n",
    "          \n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CSJmGCm9qbo_"
   },
   "outputs": [],
   "source": [
    "#Word level tf-idf\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "tfidf_vect.fit(train_df['text'])\n",
    "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
    "xvalid_tfidf =  tfidf_vect.transform(valid_x)\n",
    "np.nan_to_num(xtrain_tfidf)\n",
    "np.nan_to_num(xvalid_tfidf)\n",
    "train, test = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf, train_y, xvalid_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4VebIMpy2ssk"
   },
   "source": [
    "# Naive Bayes on Tf-iDF\n",
    "### Train: n = 49596\n",
    "    precision    recall  f1-score   support\n",
    "\n",
    "          0       0.85      0.78      0.81     20069\n",
    "          1       0.85      0.94      0.89     28448\n",
    "    avg / total       0.83      0.85      0.84     49596\n",
    "    \n",
    "###        Valid: n = 12399\n",
    "\n",
    "    precision    recall  f1-score   support\n",
    "\n",
    "          0       0.84      0.76      0.80      5047\n",
    "          1       0.84      0.93      0.88      7087\n",
    "       avg / total       0.82      0.84      0.83     12399"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lMqCOMBtqX-n"
   },
   "outputs": [],
   "source": [
    "print(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GSXLQdj78eiF"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ngram level tf-idf \n",
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "tfidf_vect_ngram.fit(trainDF['text'])\n",
    "xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_x)\n",
    "xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(valid_x)\n",
    "\n",
    "# characters level tf-idf\n",
    "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "tfidf_vect_ngram_chars.fit(trainDF['text'])\n",
    "xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(train_x) \n",
    "xvalid_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(valid_x) "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CS229BotDectector.ipynb",
   "private_outputs": true,
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
