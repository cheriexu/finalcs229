{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /home/shared/anaconda3/lib/python3.6/site-packages\n",
      "Requirement already satisfied: keras in /home/shared/anaconda3/lib/python3.6/site-packages\n",
      "Requirement already satisfied: numpy in /home/shared/anaconda3/lib/python3.6/site-packages\n",
      "Requirement already satisfied: xgboost in /home/shared/anaconda3/lib/python3.6/site-packages\n",
      "Requirement already satisfied: sklearn in /home/shared/anaconda3/lib/python3.6/site-packages\n",
      "Requirement already satisfied: nltk in /home/shared/anaconda3/lib/python3.6/site-packages\n",
      "Requirement already satisfied: textblob in /home/shared/anaconda3/lib/python3.6/site-packages\n",
      "Requirement already satisfied: python-dateutil>=2 in /home/shared/anaconda3/lib/python3.6/site-packages (from pandas)\n",
      "Requirement already satisfied: pytz>=2011k in /home/shared/anaconda3/lib/python3.6/site-packages (from pandas)\n",
      "Requirement already satisfied: scipy>=0.14 in /home/shared/anaconda3/lib/python3.6/site-packages (from keras)\n",
      "Requirement already satisfied: pyyaml in /home/shared/anaconda3/lib/python3.6/site-packages (from keras)\n",
      "Requirement already satisfied: six>=1.9.0 in /home/shared/anaconda3/lib/python3.6/site-packages (from keras)\n",
      "Requirement already satisfied: scikit-learn in /home/shared/anaconda3/lib/python3.6/site-packages (from sklearn)\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas keras numpy xgboost sklearn nltk textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda install pytorch torchvision -c pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Data and set Test Train Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'IRAhandle_tweets_9.csv'\n",
      "b'IRAhandle_tweets_4.csv'\n",
      "b'IRAhandle_tweets_8.csv'\n",
      "b'IRAhandle_tweets_7.csv'\n",
      "b'IRAhandle_tweets_11.csv'\n",
      "b'IRAhandle_tweets_10.csv'\n",
      "b'IRAhandle_tweets_3.csv'\n",
      "b'IRAhandle_tweets_6.csv'\n",
      "b'IRAhandle_tweets_12.csv'\n",
      "b'IRAhandle_tweets_2.csv'\n",
      "b'IRAhandle_tweets_1.csv'\n",
      "b'README.md'\n",
      "b'IRAhandle_tweets_13.csv'\n",
      "b'IRAhandle_tweets_5.csv'\n",
      "2116867 2946207\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "82661"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "\n",
    "import pandas as pd, xgboost, numpy as np, textblob, string\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import layers, models, optimizers\n",
    "import os\n",
    "\n",
    "real_users = pd.read_csv('/home/cheriexu/genuine_accounts/users.csv',low_memory = False)\n",
    "real_tweets = pd.read_csv('/home/cheriexu/genuine_accounts/tweets.csv',low_memory = False)\n",
    "real_df = real_tweets.merge(real_users, on='id',how = 'outer', suffixes = ('_tweets','_users')) #Merging the user and tweets\n",
    "real_df['labels']=pd.Series(np.ones(len(real_df['id']))) #Creating a new column to indicate that this is human data\n",
    "#Get Bot data \n",
    "directory = os.fsencode('/home/cheriexu/russian-troll')\n",
    "troll_data = pd.DataFrame()\n",
    "for file in os.listdir(directory):\n",
    "  print(file)\n",
    "  filename = os.fsdecode(file)\n",
    "  if filename.endswith(\".csv\"):\n",
    "    botdata = pd.read_csv('/home/cheriexu/russian-troll/'+filename,low_memory = False)\n",
    "    troll_data = troll_data.append(botdata) #append together\n",
    "troll_data = troll_data.rename(index = str, columns = {\"publish_date\":\"date\"})\n",
    "\n",
    "\n",
    "#Downsample to \n",
    "frac = 60       #Factor to Downsample due to runtime constraints \n",
    "troll_data['labels']=pd.Series(np.zeros(len(botdata)))\n",
    "#Limiting too only English language tweets\n",
    "english_bot = troll_data[troll_data['language']  == 'English']\n",
    "troll_data['language'].unique()\n",
    "print(len(english_bot),len(troll_data)) #Class imbalance though, we need more spam bot results\n",
    "train_df = pd.DataFrame() #Create train data frame for splitting later\n",
    "real_df = real_df.rename(index = str, columns = {\"timestamp_tweets\": \"date\"})\n",
    "real_df= real_df.dropna()\n",
    "train_df = train_df.append(real_df[['text','labels','date']].sample(int(len(real_df)/frac)))\n",
    "english_bot = english_bot.rename(index=str, columns={\"content\": \"text\"})\n",
    "english_bot = english_bot.dropna()\n",
    "train_df = train_df.append(english_bot[['text','labels','date']].sample(int(len(english_bot)/frac)))\n",
    "len(train_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess Cells and Clean Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "# Others\n",
    "import nltk\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/cheriexu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    ## Remove puncuation\n",
    "    text = str(text)\n",
    "    text = text.translate(string.punctuation)\n",
    "    \n",
    "    ## Convert words to lower case and split them\n",
    "    text = text.lower().split()\n",
    "    \n",
    "    ## Remove stop words\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    text = [w for w in text if not w in stops and len(w) >= 3]\n",
    "    \n",
    "    text = \" \".join(text)\n",
    "    ## Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    ## Stemming\n",
    "    text = text.split()\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    stemmed_words = [stemmer.stem(word) for word in text]\n",
    "    text = \" \".join(stemmed_words)\n",
    "    return text\n",
    "\n",
    "train_df['text'] = train_df['text'].map(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Vocab and Create Tokenized array\n",
    "vocabulary_size = 20000\n",
    "tokenizer = Tokenizer(num_words= vocabulary_size)\n",
    "tokenizer.fit_on_texts(train_df['text'])\n",
    "sequences = tokenizer.texts_to_sequences(train_df['text'])\n",
    "data = pad_sequences(sequences, maxlen=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = preprocessing.LabelEncoder()\n",
    "y = encoder.fit_transform(train_df['labels'])\n",
    "\n",
    "#valid_y = encoder.fit_transform(valid_y)\n",
    "#np.array(train_df['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 28276 samples, validate on 18851 samples\n",
      "Epoch 1/3\n",
      "28276/28276 [==============================] - 82s 3ms/step - loss: 0.0071 - acc: 0.9996 - val_loss: 7.4283e-06 - val_acc: 1.0000\n",
      "Epoch 2/3\n",
      "28276/28276 [==============================] - 81s 3ms/step - loss: 5.6151e-06 - acc: 1.0000 - val_loss: 3.5870e-06 - val_acc: 1.0000\n",
      "Epoch 3/3\n",
      "11328/28276 [===========>..................] - ETA: 43s - loss: 3.3664e-06 - acc: 1.0000"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(20000, 100, input_length=50))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "## Fit the model\n",
    "model.fit(data, y, validation_split=0.4, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(train_df['text'], train_df['labels'], test_size = .2)\n",
    "train_df['text'] = train_df['text'].fillna(' ')\n",
    "train_x = train_x.fillna(' ')\n",
    "valid_x = valid_x.fillna(' ') \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
