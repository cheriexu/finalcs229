{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CS229BotDectectorANN.ipynb","version":"0.3.2","provenance":[{"file_id":"1zF6484pwIt3QB_P6K8HTlHZsP-liRimF","timestamp":1542358710917}],"private_outputs":true,"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"cG1fn2Wb802F","colab_type":"text"},"cell_type":"markdown","source":["## THIS IS THE HOLY GRAIL THAT IS GONNA SAVE OUR ASSES \n","(https://www.analyticsvidhya.com/blog/2018/04/a-comprehensive-guide-to-understand-and-implement-text-classification-in-python/)"]},{"metadata":{"id":"gPihiRLWwE-n","colab_type":"text"},"cell_type":"markdown","source":["## Constructing a shallow ANN for classification\n","##MAYBE add in Sentiment Analysis as feature "]},{"metadata":{"id":"bGcJCjeXFZbz","colab_type":"code","colab":{}},"cell_type":"code","source":["!pip3 install pandas keras numpy xgboost sklearn nltk textblob"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Lgs6YzPuFunI","colab_type":"code","colab":{}},"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"iMwZVdN8h1tN","colab_type":"code","colab":{}},"cell_type":"code","source":["cd drive/'My Drive'/CS229"],"execution_count":0,"outputs":[]},{"metadata":{"id":"convk9PYFt6p","colab_type":"text"},"cell_type":"markdown","source":["### Get genuine human tweets"]},{"metadata":{"id":"Rya8yaGT_-Ht","colab_type":"code","colab":{}},"cell_type":"code","source":["from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n","from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n","from sklearn import decomposition, ensemble\n","\n","import pandas as pd, xgboost, numpy as np, textblob, string\n","from keras.preprocessing import text, sequence\n","from keras import layers, models, optimizers\n","\n","real_users = pd.read_csv('./GenuineData/ga/users.csv',low_memory = False)\n","real_tweets = pd.read_csv('./GenuineData/ga/tweets.csv',low_memory = False)\n","real_df = real_tweets.merge(real_users, on='id',how = 'outer', suffixes = ('_tweets','_users')) #Merging the user and tweets\n","real_df['labels']=pd.Series(np.ones(len(real_df['id']))) #Creating a new column to indicate that this is human data\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"QzJ7u2mrnDCA","colab_type":"code","colab":{}},"cell_type":"code","source":["real_df['timestamp_tweets']"],"execution_count":0,"outputs":[]},{"metadata":{"id":"TcyRaSYtsqwh","colab_type":"code","colab":{}},"cell_type":"code","source":["import tensorflow as tf\n","tf.test.gpu_device_name() #We good using GPU"],"execution_count":0,"outputs":[]},{"metadata":{"id":"kQEzdJPmzkIJ","colab_type":"text"},"cell_type":"markdown","source":["### Import Bot Dataset from this point on \n","(https://github.com/fivethirtyeight/russian-troll-tweets/)"]},{"metadata":{"id":"dwpQbQrvoAxw","colab_type":"code","colab":{}},"cell_type":"code","source":["import os\n","directory = os.fsencode('./russian-troll-tweets')\n","troll_data = pd.DataFrame()\n","for file in os.listdir(directory):\n","  print(file)\n","  filename = os.fsdecode(file)\n","  if filename.endswith(\".csv\"):\n","    botdata = pd.read_csv('./russian-troll-tweets/'+filename,low_memory = False)\n","    troll_data = troll_data.append(botdata) #append together"],"execution_count":0,"outputs":[]},{"metadata":{"id":"dKTste3fnUKA","colab_type":"code","colab":{}},"cell_type":"code","source":["troll_data = troll_data.rename(index = str, columns = {\"publish_date\":\"date\"})\n","#troll_data[troll_data['publish_date'].year >=2017]\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"bdCjl1rEug7x","colab_type":"code","colab":{}},"cell_type":"code","source":["frac = 80       #Factor to Downsample due to runtime constraints \n","troll_data['labels']=pd.Series(np.zeros(len(botdata)))\n","#Limiting too only English language tweets\n","english_bot = troll_data[troll_data['language']  == 'English']\n","troll_data['language'].unique()\n","print(len(english_bot),len(troll_data)) #Class imbalance though, we need more spam bot results\n","train_df = pd.DataFrame() #Create train data frame for splitting later\n","real_df = real_df.rename(index = str, columns = {\"timestamp_tweets\": \"date\"})\n","train_df = train_df.append(real_df[['text','labels','date']].sample(int(len(real_df)/frac)))\n","english_bot = english_bot.rename(index=str, columns={\"content\": \"text\"})\n","train_df = train_df.append(english_bot[['text','labels','date']].sample(int(len(english_bot)/frac)))\n","#Downsample to 12.5%\n","len(train_df)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"SGFE7ANbr2AE","colab_type":"code","colab":{}},"cell_type":"code","source":["train_df.head()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"plG4IxOc8OLd","colab_type":"text"},"cell_type":"markdown","source":["### Split train test dataset:"]},{"metadata":{"id":"6qk21OCf8Nee","colab_type":"code","colab":{}},"cell_type":"code","source":["# split the dataset into training and validation datasets \n","train_x, valid_x, train_y, valid_y = model_selection.train_test_split(train_df['text'], train_df['labels'], test_size = .2)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"--xxycI-8Ufm","colab_type":"text"},"cell_type":"markdown","source":["### Create Count Vectors for NB"]},{"metadata":{"id":"RFT2IwTl8XHz","colab_type":"code","colab":{}},"cell_type":"code","source":["#fill in nan to make sklearn happy\n","train_df['text'] = train_df['text'].fillna(' ')\n","train_x = train_x.fillna(' ')\n","valid_x = valid_x.fillna(' ') \n","#ngram\n","tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n","tfidf_vect_ngram.fit(train_df['text'])\n","xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_x)\n","xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(valid_x)\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Y3gHFXbN6j4w","colab_type":"code","colab":{}},"cell_type":"code","source":["encoder = preprocessing.LabelEncoder()\n","train_y = encoder.fit_transform(train_y)\n","valid_y = encoder.fit_transform(valid_y)\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"H3WgYHt29Bf9","colab_type":"code","colab":{}},"cell_type":"code","source":["#xtrain_count.shape\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"PtXxa9Fm3u_A","colab_type":"text"},"cell_type":"markdown","source":["### Shallow Neural Net that uses count vector and tf-IDF and possible sentiment"]},{"metadata":{"id":"_kIvH_xv3LOj","colab_type":"text"},"cell_type":"markdown","source":["### Train Model of select type and make predictions. "]},{"metadata":{"id":"VndDnk8G3Joq","colab_type":"code","colab":{}},"cell_type":"code","source":["def train_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n","    classifier.fit(feature_vector_train, label)\n","    pred_train = classifier.predict(feature_vector_train)\n","    predictions = classifier.predict(feature_vector_valid)\n","    if is_neural_net:\n","        predictions = predictions.argmax(axis=-1)\n","        pred_train = pred_train.argmax(axis = -1)\n","\n","    return metrics.classification_report(label, pred_train), metrics.classification_report(valid_y, predictions)#confusion matrix on validation matrix\n","  #metrics.confusion_matrix(label, pred_test), metrics.confusion_matrix(valid_y, predictions)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"bHQeogEY3uCO","colab_type":"code","colab":{}},"cell_type":"code","source":["def create_model_architecture(input_size):\n","    # create input layer \n","    input_layer = layers.Input((input_size, ), sparse=True)\n","    \n","    # create hidden layer\n","    hidden_layer1 = layers.Dense(100, activation='relu')(input_layer)\n","    hidden_layer2 = layers.Dense(20, activation=\"relu\")(hidden_layer1)\n","    \n","    # create output layer\n","    output_layer = layers.Dense(1, activation=\"sigmoid\")(hidden_layer2)\n","\n","    classifier = models.Model(inputs = input_layer, outputs = output_layer)\n","    classifier.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n","    return classifier \n","\n","classifier = create_model_architecture(xtrain_tfidf_ngram.shape[1])\n","train,test = train_model(classifier, xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram, is_neural_net=True)\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"MONyOvzbsadY","colab_type":"code","colab":{}},"cell_type":"code","source":["#USE THIS INSTEAD\n","#https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html\n","sklearn.neural_network.MLPClassifier"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Zq3hRU6_iow0","colab_type":"code","colab":{}},"cell_type":"code","source":["print(test)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"WPRaf6ws8nNE","colab_type":"text"},"cell_type":"markdown","source":["### Naive Bayes"]},{"metadata":{"id":"gy1IhwsvUfU1","colab_type":"text"},"cell_type":"markdown","source":["## Build 2 layer Neural Net, tweek later"]},{"metadata":{"id":"hs8oEaeF8oLG","colab_type":"code","colab":{}},"cell_type":"code","source":["# Naive Bayes on Count Vectors\n","np.nan_to_num(xtrain_count)\n","np.nan_to_num(xvalid_count)\n","conf_test, conf_valid = train_model(naive_bayes.MultinomialNB(), xtrain_count, train_y, xvalid_count)\n","print (conf_test[0:2])\n","print(conf_valid[0:2])\n","\n","'''# Naive Bayes on Word Level TF IDF Vectors\n","accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf, train_y, xvalid_tfidf)\n","print \"NB, WordLevel TF-IDF: \", accuracy\n","\n","# Naive Bayes on Ngram Level TF IDF Vectors\n","accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n","print \"NB, N-Gram Vectors: \", accuracy\n","\n","# Naive Bayes on Character Level TF IDF Vectors\n","accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\n","'''"],"execution_count":0,"outputs":[]},{"metadata":{"id":"6YNG6mAPtjUv","colab_type":"code","colab":{}},"cell_type":"code","source":["print (conf_test[:2])\n","print(conf_valid[:2])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"CSJmGCm9qbo_","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"lMqCOMBtqX-n","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"425Y0ogb8cCZ","colab_type":"text"},"cell_type":"markdown","source":["tf-idf: don't run yet"]},{"metadata":{"id":"GSXLQdj78eiF","colab_type":"code","colab":{}},"cell_type":"code","source":["# word level tf-idf\n","tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n","tfidf_vect.fit(trainDF['text'])\n","xtrain_tfidf =  tfidf_vect.transform(train_x)\n","xvalid_tfidf =  tfidf_vect.transform(valid_x)\n","\n","# ngram level tf-idf \n","tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n","tfidf_vect_ngram.fit(trainDF['text'])\n","xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_x)\n","xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(valid_x)\n","\n","# characters level tf-idf\n","tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n","tfidf_vect_ngram_chars.fit(trainDF['text'])\n","xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(train_x) \n","xvalid_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(valid_x) "],"execution_count":0,"outputs":[]}]}